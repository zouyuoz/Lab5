{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2025 DL Lab5: Object Detection on Pascal VOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, please put **your name** and **SID** in following format: <br>\n",
    "Hi I'm 陸仁賈, 314831000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm XXX, XXXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project focuses on object detection using the Pascal VOC dataset. \n",
    "\n",
    "The goal is to identify and locate various objects within images by training and evaluating detection models.\n",
    " \n",
    "The dataset provides annotated images across multiple categories, making it a standard benchmark for evaluating object detection performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition\n",
    "Kaggle is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish datasets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n",
    "\n",
    "This assignment use kaggle to calculate your grade.  \n",
    "Please use this [**LINK**](https://www.kaggle.com/t/3fd493e454a744bdacc7f2918f9a2605) to join the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Data\n",
    "\n",
    "Unzip `dataset.zip` \n",
    "\n",
    "+ `vocall_test.txt` : list for the training set\n",
    "+ `vocall_test.txt` : list for the validation set\n",
    "+ `vocall_test.txt` : list for the test set\n",
    "+ `image/` : contains all images.\n",
    "\n",
    "\n",
    "The train set contains 8,218 images, the val set contains 3,823 images, and the test set contains 8,920 images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You are allowed to use a **backbone model**, but only those available from the **timm package** (https://huggingface.co/timm/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.amp import autocast, GradScaler\n",
    "from src.yolo import getODmodel\n",
    "from yolo_loss import YOLOv3Loss\n",
    "from src.dataset import VocDetectorDataset, train_data_pipelines, test_data_pipelines, collate_fn\n",
    "from src.eval_voc import evaluate\n",
    "from src.config import GRID_SIZES, ANCHORS\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####hyperparameters#####\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 50\n",
    "batch_size = 40\n",
    "learning_rate = 1e-3\n",
    "lambda_coord=5.0\n",
    "lambda_obj=1.0\n",
    "lambda_noobj=0.5\n",
    "lambda_class=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "file_root_train = './dataset/image/'\n",
    "annotation_file_train = './dataset/vocall_train.txt'\n",
    "file_root_val = './dataset/image/'\n",
    "annotation_file_val = './dataset/vocall_val.txt'\n",
    " # Data paths\n",
    "file_root_train = './dataset/image/'\n",
    "annotation_file_train = './dataset/vocall_train.txt'\n",
    "file_root_val = './dataset/image/'\n",
    "annotation_file_val = './dataset/vocall_val.txt'\n",
    "\n",
    "# Create datasets\n",
    "print('Loading datasets...')\n",
    "train_dataset = VocDetectorDataset(\n",
    "    root_img_dir=file_root_train,\n",
    "    dataset_file=annotation_file_train,\n",
    "    train=True,\n",
    "    transform=train_data_pipelines,\n",
    "    grid_sizes=GRID_SIZES,\n",
    "    encode_target=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "print(f'Loaded {len(train_dataset)} train images')\n",
    "\n",
    "val_dataset = VocDetectorDataset(\n",
    "    root_img_dir=file_root_val,\n",
    "    dataset_file=annotation_file_val,\n",
    "    train=False,\n",
    "    transform=test_data_pipelines,\n",
    "    grid_sizes=GRID_SIZES,\n",
    "    encode_target=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "#for computing val maps\n",
    "eval_dataset = VocDetectorDataset(\n",
    "    root_img_dir=file_root_val,\n",
    "    dataset_file=annotation_file_val,\n",
    "    train=False,\n",
    "    transform=test_data_pipelines,\n",
    "    grid_sizes=GRID_SIZES,\n",
    "    encode_target=False,\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "print(f'Loaded {len(val_dataset)} val images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only backbone model on timm is acceptable (https://huggingface.co/timm/models).\n",
    "### You can modify model name in yolo class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_network_path = None #'checkpoints/best_detector.pth' \n",
    "pretrained = True\n",
    "model = getODmodel(pretrained=pretrained).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some training utils, use mix precision if valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss and optimizer\n",
    "criterion = YOLOv3Loss(lambda_coord, lambda_obj, lambda_noobj, lambda_class, ANCHORS).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = GradScaler(enabled=use_amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print('\\nStarting training...')\n",
    "torch.cuda.empty_cache()\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(f'\\n\\nStarting epoch {epoch + 1} / {num_epochs}')\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        images = images.to(device)\n",
    "        target = [t.to(device) for t in target]\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(\"cuda\", enabled=use_amp):\n",
    "            pred = model(images)\n",
    "            # pred and target are lists of each scales\n",
    "            loss_dict = criterion(pred, target)\n",
    "        # Backward pass with mixed precision support\n",
    "        scaler.scale(loss_dict['total']).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # Print progress\n",
    "        if i % 50 == 0:\n",
    "            outstring = f'Epoch [{epoch+1}/{num_epochs}], Iter [{i+1}/{len(train_loader)}], Loss: '\n",
    "            outstring += ', '.join(f\"{key}={val :.3f}\" for key, val in loss_dict.items())\n",
    "            print(outstring)\n",
    "    lr_scheduler.step()\n",
    "    learning_rate = lr_scheduler.get_last_lr()[0]\n",
    "    print(f'Learning Rate for this epoch: {learning_rate}')\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            target = [t.to(device) for t in target]\n",
    "            # Forward pass\n",
    "            pred = model(images)\n",
    "            loss_dict = criterion(pred, target)\n",
    "            val_loss += loss_dict['total'].item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Save best model\n",
    "    if best_val_loss > val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(f'Updating best val loss: {best_val_loss:.5f}')\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        torch.save(model.state_dict(), 'checkpoints/best_detector.pth')\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) in [5, 10, 20, 30, 40]:\n",
    "        torch.save(model.state_dict(), f'checkpoints/detector_epoch_{epoch+1}.pth')\n",
    "\n",
    "    torch.save(model.state_dict(), 'checkpoints/detector.pth')\n",
    "\n",
    "    # Evaluate on val set\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print('\\nEvaluating on validation set...')\n",
    "        val_aps = evaluate(model, eval_loader)\n",
    "        print(f'Epoch {epoch}, mAP: {np.mean(val_aps):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Result\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/3fd493e454a744bdacc7f2918f9a2605).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. Click the folder icon in the left hand side of Colab.\n",
    "2. Right click \"result.csv\". Select \"Download\"\n",
    "3. To kaggle. Click \"Submit Predictions\"\n",
    "4. Upload the result.csv\n",
    "5. System will automaticlaly calculate the accuracy of 50% dataset and publish this result to leaderboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict_test.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
